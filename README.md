# Awesome 3D Scene Understanding
This repo collects papers, docs, codes about 3D Scene Understanding for anyone who wants to do research on it. We are continuously improving the project. Welcome to PR the works (papers, repositories) that are missed by the repo. Special thanks to [Fucheng Cai](https://github.com/HITCai), [Zhe yang](https://github.com/xia-zhe), and all researchers who have contributed to this project!

## Table of Contents

- [Papers](#Papers)
  - [2024](#2024)
  - [2023](#2023)
  - [2022](#2022)
  - [2021](#2021)


## Papers

### 2024
- [[ArXiv](https://arxiv.org/abs/2405.05258)] Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving [[code](https://github.com/ldkong1205/LaserMix)]
- [[AAAI](https://arxiv.org/abs/2306.02329)] SQA3D: Situated Question Answering in 3D Scenes [[code](https://sqa3d.github.io/)]
- [[AAAI](https://arxiv.org/abs/2402.15933)] Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA [[code](https://github.com/matthewdm0816/BridgeQA)]
- [[CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Towards_Learning_a_Generalist_Model_for_Embodied_Navigation_CVPR_2024_paper.html)] Towards Learning a Generalist Model for Embodied Navigation [[code](https://github.com/LaVi-Lab/NaviLLM)]
- [[CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_HUGS_Holistic_Urban_3D_Scene_Understanding_via_Gaussian_Splatting_CVPR_2024_paper.pdf)] HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting [[code](https://github.com/hyzhou404/HUGS)]
- [[CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_RegionPLC_Regional_Point-Language_Contrastive_Learning_for_Open-World_3D_Scene_Understanding_CVPR_2024_paper.pdf)] RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding [[code](https://github.com/CVMI-Lab/PLA)]
  
### 2023
- [[CVPRW](https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/html/Parelli_CLIP-Guided_Vision-Language_Pre-Training_for_Question_Answering_in_3D_Scenes_CVPRW_2023_paper.html)] CLIP-Guided Vision-Language Pre-Training for Question Answering in 3D Scenes [[code](https://github.com/alexdelitzas/3d-vqa)]
- [[CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_OpenScene_3D_Scene_Understanding_With_Open_Vocabularies_CVPR_2023_paper.pdf)] OpenScene: 3D Scene Understanding with Open Vocabularies [[code](https://github.com/pengsongyou/openscene)]
- [[CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.pdf)] CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP [[code](https://github.com/runnanchen/CLIP2Scene)]
- [[CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_PLA_Language-Driven_Open-Vocabulary_3D_Scene_Understanding_CVPR_2023_paper.pdf)] PLA: Language-Driven Open-Vocabulary 3D Scene Understanding [[code](https://github.com/CVMI-Lab/PLA)]
### 2022
- [[CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Azuma_ScanQA_3D_Question_Answering_for_Spatial_Scene_Understanding_CVPR_2022_paper.html)] ScanQA: 3D Question Answering for Spatial Scene Understanding [[code](https://github.com/ATR-DBI/ScanQA)]
### 2021
- [[CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Hou_Exploring_Data-Efficient_3D_Scene_Understanding_With_Contrastive_Scene_Contexts_CVPR_2021_paper.pdf)] Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts 
